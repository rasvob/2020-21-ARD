{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for Big Data - Exercise 2\n",
    "This lecture is about storing and restoring model, application o fthe checkpoints.\n",
    "\n",
    "We use validating sets, and evaluate our models on FashionMnist dataset.\n",
    "\n",
    "Also we will try the TransferLearning simple/educational example where we use one model trained on a dataset and retrained only the final layer for different dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/2020-21-ARD/blob/master/abd_02.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/2020-21-ARD/blob/master/abd_02.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np #numpy\n",
    "import tensorflow.compat.v2 as tf #use tensorflow v2 as a main \n",
    "import tensorflow.keras as keras # required for high level applications\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some utility functions\n",
    "Here is some functions we will use later several times\n",
    "* **show_history** - show history of the **fit** method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset load\n",
    "Importig dataset **FashonMinst** a collection of images 28x28 grayscale of typical clothes and accesories.\n",
    "Dataset is:\n",
    "* downloaded\n",
    "* splitted into train and test set\n",
    "* converted from the range <0,255> into <0, 1>\n",
    "* *train* is splitted into *train* and *validation* set \n",
    "* class names are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist is the basic dataset with handwritten digits\n",
    "dataset = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# data from any dataset are loaded using the load_Data function\n",
    "(train_x, train_y), (test_x, test_y) = dataset.load_data()\n",
    "\n",
    "train_x, test_x = train_x/255.0, test_x/255.0\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# the data are in the form of 28x28 pixes with values 0-255.\n",
    "print('Train data shape: ', train_x.shape, train_y.shape)\n",
    "print('Validation data shape: ', valid_x.shape, valid_y.shape)\n",
    "print('Test data shape:  ', test_x.shape, test_y.shape)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boo']\n",
    "class_count = len(class_names)\n",
    "print('Class count:', class_count, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show first 25 images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_x[i], cmap=plt.cm.gray)\n",
    "    plt.xlabel(class_names[train_y[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the model\n",
    "The base model is defined as *Sequential* with three layers, similarly to the previous lecture.\n",
    "\n",
    "Summarization of the model and compilation is done similarly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),# Flatten module flatten the multidimension input into single vector 28x28 = 784 float numbers\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # standard dense-fully connected layer with the rectified lineaar function as an activation\n",
    "    keras.layers.Dense(class_count, activation=tf.nn.softmax), # another fully-connected layer with softmax activation function\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model for defined number of epochs.\n",
    "Show the history of learning, evaluate the efficiency of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=10)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualisation of the evaluation\n",
    "The function *predict* return the prediction for the defined input - a test set.\n",
    "\n",
    "The prediction in this case is a vector of probabilities foe each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x)\n",
    "print(predictions[0], np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualization of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set accuracy\n",
    "accuracy_score(y_pred=pred_y, y_true=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_pred=pred_y, y_true=test_y)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_norm = normalize(conf_matrix, axis=1, norm='l1')\n",
    "conf_matrix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(class_count,class_count))\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        text = plt.text(j, i, \"{:d}\".format(conf_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "plt.imshow(conf_matrix)\n",
    "plt.tight_layout()\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(class_count,class_count))\n",
    "for i in range(conf_matrix_norm.shape[0]):\n",
    "    for j in range(conf_matrix_norm.shape[1]):\n",
    "        text = plt.text(j, i, \"{:.3f}\".format(conf_matrix_norm[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "plt.imshow(conf_matrix_norm)\n",
    "plt.tight_layout()\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load of the model\n",
    "The model I/O works as follows. The topology of the model is usually stored separately from the weights. The reason is that the topology is usually used once, but the weights are used frequently and they develop during the learning process. \n",
    "\n",
    "The topology is stored as JSON file format, as is demonstrated later in more human readable form. As you may see, the document is very simple and just summarize the layers and its types - app parameters set during the model creation.\n",
    "\n",
    "The weights are stored using the HDF5 file format a format developed for fast unlimited storage with portable and distributable capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = 'model.json'\n",
    "file_weight = 'model.h5'\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(file_model, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(file_weight)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"class_name\": \"Sequential\",\n",
    "    \"config\": {\n",
    "        \"name\": \"sequential\",\n",
    "        \"layers\": [\n",
    "            {\n",
    "                \"class_name\": \"Flatten\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"flatten\",\n",
    "                    \"trainable\": true,\n",
    "                    \"batch_input_shape\": [\n",
    "                        null,\n",
    "                        28,\n",
    "                        28\n",
    "                    ],\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"data_format\": \"channels_last\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class_name\": \"Dense\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"dense\",\n",
    "                    \"trainable\": true,\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"units\": 32,\n",
    "                    \"activation\": \"relu\",\n",
    "                    \"use_bias\": true,\n",
    "                    \"kernel_initializer\": {\n",
    "                        \"class_name\": \"GlorotUniform\",\n",
    "                        \"config\": {\n",
    "                            \"seed\": null\n",
    "                        }\n",
    "                    },\n",
    "                    \"bias_initializer\": {\n",
    "                        \"class_name\": \"Zeros\",\n",
    "                        \"config\": {}\n",
    "                    },\n",
    "                    \"kernel_regularizer\": null,\n",
    "                    \"bias_regularizer\": null,\n",
    "                    \"activity_regularizer\": null,\n",
    "                    \"kernel_constraint\": null,\n",
    "                    \"bias_constraint\": null\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class_name\": \"Dense\",\n",
    "                \"config\": {\n",
    "                    \"name\": \"dense_1\",\n",
    "                    \"trainable\": true,\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"units\": 10,\n",
    "                    \"activation\": \"softmax\",\n",
    "                    \"use_bias\": true,\n",
    "                    \"kernel_initializer\": {\n",
    "                        \"class_name\": \"GlorotUniform\",\n",
    "                        \"config\": {\n",
    "                            \"seed\": null\n",
    "                        }\n",
    "                    },\n",
    "                    \"bias_initializer\": {\n",
    "                        \"class_name\": \"Zeros\",\n",
    "                        \"config\": {}\n",
    "                    },\n",
    "                    \"kernel_regularizer\": null,\n",
    "                    \"bias_regularizer\": null,\n",
    "                    \"activity_regularizer\": null,\n",
    "                    \"kernel_constraint\": null,\n",
    "                    \"bias_constraint\": null\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"keras_version\": \"2.2.4-tf\",\n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model\n",
    "The model is also loadable separatelly. Topology and weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model need to be compiled to be applicable to the data.As you may see the summary of the model is the same as was for the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model has the same results like the stored one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = loaded_model.evaluate(test_x, test_y)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks for Lecture \n",
    "1. The task here will be focused on the usage of some [CallBacks](https://keras.io/callbacks/) from the keras API which is intended to monitor learning process. The major ones are *ModelCheckpoint* and *EarlyStopping*.\n",
    "2. [Transfer learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) is a method for reusing a prepared model from some type of dataset to new dataset. The usage of that model has many advantages. Lets go throug the link attached and read the description.\n",
    "   * The problem will be demonstrated in the MNIST dataset (from the previous lecture).\n",
    "   * The model trained on the FashionMnist will be reused on the Mnist dataset.\n",
    "   * The first layers will be deactivated for training and model will be compiled.\n",
    "   * The last layer will be trained on the new dataset and the result uses the knowledge from the previous learning.\n",
    "3. Desing a model which will be able to classify **FashionMnist** with accuracy higher than **92%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization (see [this](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c) for more details)\n",
    "- If we have some two input variables with scales from 0 to 1 and from 0 to 1000 we can normalize them\n",
    "    - Larger scale does not mean that the varible is more important than the other one\n",
    "    - It's clear that without normalization one weight will be very high and other very low to balance the scale difference\n",
    "    - This leads to slow gradient descent convergence - we need small learning rates\n",
    "        - Loss function space is not smooth - gradients may oscillate back and forth before finding optimum\n",
    "        \n",
    "- This issue is present in hidden layers as well due to the mini-batch learning\n",
    "    - Each batch is different from others from distributions point of view\n",
    "    - Covariate shift effect - distribution of data is constantly changing during training\n",
    "        - E.g. We train model on images of black cats - we learn how to map input X to output Y\n",
    "        - If we now use images of colored cats for testing purposes the model will fail - input X changed thus learned mapping is invalid \n",
    "        - This shift happens internally with mini-batches all the time\n",
    "    - Batch norm. standardize activation values to have same mean and variance among batch\n",
    "    - Slight regularization - it adds some noise to each hidden layer’s activations so less overfitting\n",
    "    - We can use higher learning rates because batch normalization makes sure that there’s no activation that’s gone really high or really low\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
