{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilthBvnZCQto"
   },
   "source": [
    "# Algorithms for Big Data - Exercise 7\n",
    "This lecture is focused on the more advanced examples of the RNN usage for text data anylysis.\n",
    "\n",
    "We will deal with the sentiment analysis task using Twitter data.\n",
    "\n",
    "You can download the dataset from [this link](https://github.com/MohamedAfham/Twitter-Sentiment-Analysis-Supervised-Learning/tree/master/Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fi2Jwhs35Itq"
   },
   "source": [
    "[Open in Google colab](https://colab.research.google.com/github/rasvob/2020-21-ARD/blob/master/abd_07.ipynb)\n",
    "[Download from Github](https://github.com/rasvob/2020-21-ARD/blob/master/abd_07.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg # images\n",
    "import numpy as np #numpy\n",
    "import seaborn as sns\n",
    "import tensorflow.compat.v2 as tf #use tensorflow v2 as a main \n",
    "import tensorflow.keras as keras # required for high level applications\n",
    "from sklearn.model_selection import train_test_split # split for validation sets\n",
    "from sklearn.preprocessing import normalize # normalization of the matrix\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata, re, string\n",
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.figure()\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.epoch, history.history[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(keras.layers.Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "keras.utils.get_custom_objects().update({'mish': Mish(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/rasvob/2020-21-ARD/master/datasets/train_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the classification task is highly imbalanced, because we have only 2242 negative tweets compared with positive one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='label', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df.tweet.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the sentences are of similar lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='length'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='label', y='length', data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that the text data are full of noise\n",
    "\n",
    "- Social posts suffer the most from this effect\n",
    "- The text is full of hashtags, emojis, @mentions and so on\n",
    "- These parts usually don't influence the sentiment score by much\n",
    "- Although most advanced models usually extract even this features because e.g. emojis can help you with the sarcasm understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "---------\n",
      "@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
      "---------\n",
      "  bihday your majesty\n",
      "---------\n",
      "#model   i love u take with u all the time in urð±!!! ðððð",
      "ð¦ð¦ð¦  \n",
      "---------\n",
      " factsguide: society now    #motivation\n",
      "---------\n",
      "[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  \n",
      "---------\n",
      " @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦\n",
      "---------\n",
      "the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl\n",
      "---------\n",
      "we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦ \n",
      "---------\n",
      " @user @user welcome here !  i'm   it's so #gr8 ! \n",
      "---------\n",
      " â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for x in df_train.loc[:10, 'tweet']:\n",
    "    print(x)\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”,\n",
    "\n",
    "## Lemmatization \n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "\n",
    "Examples of lemmatization:\n",
    "\n",
    "- rocks : rock\n",
    "- corpora : corpus\n",
    "- better : good\n",
    "\n",
    "## Both techiques can be used in the preprocessing pipeline\n",
    "You have to decide if it is beneficial to you, because this steps leads to some generalization of the data by itself. You will definitely lose some pieces of the information. If you use some form of embedding like Word2Vec or Glove, it is better to skip this steps because the embedding vocabulary skipped it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "# words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "#    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return tweet_blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize sentences and remove puncuation by TextBlob library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words'] = df['tweet'].apply(form_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Words</th>\n",
       "      <th>Words_normalized</th>\n",
       "      <th>Words_normalized_no_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "      <td>[when, a, father, is, dysfunctional, and, is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "      <td>[thanks, for, lyft, credit, i, ca, n't, use, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                               Words  \\\n",
       "0  [user, when, a, father, is, dysfunctional, and...   \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                    Words_normalized  \\\n",
       "0  [user, when, a, father, is, dysfunctional, and...   \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                            Words_normalized_no_user  \n",
       "0  [when, a, father, is, dysfunctional, and, is, ...  \n",
       "1  [thanks, for, lyft, credit, i, ca, n't, use, c...  \n",
       "2                            [bihday, your, majesty]  \n",
       "3  [model, i, love, u, take, with, u, all, the, t...  \n",
       "4             [factsguide, society, now, motivation]  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize sentences \n",
    "- We want only ascii, lowercase and no numbers\n",
    "\n",
    "## You can experiments with different preprocess steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words_normalized'] = df['Words'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Words</th>\n",
       "      <th>Words_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                               Words  \\\n",
       "0  [user, when, a, father, is, dysfunctional, and...   \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                    Words_normalized  \n",
       "0  [user, when, a, father, is, dysfunctional, and...  \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...  \n",
       "2                            [bihday, your, majesty]  \n",
       "3  [model, i, love, u, take, with, u, all, the, t...  \n",
       "4             [factsguide, society, now, motivation]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the 'user' word from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words_normalized_no_user'] = df['Words_normalized'].apply(lambda x: [y for y in x if 'user' not in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Words</th>\n",
       "      <th>Words_normalized</th>\n",
       "      <th>Words_normalized_no_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "      <td>[when, a, father, is, dysfunctional, and, is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "      <td>[thanks, for, lyft, credit, i, ca, n't, use, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                               Words  \\\n",
       "0  [user, when, a, father, is, dysfunctional, and...   \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                    Words_normalized  \\\n",
       "0  [user, when, a, father, is, dysfunctional, and...   \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                            Words_normalized_no_user  \n",
       "0  [when, a, father, is, dysfunctional, and, is, ...  \n",
       "1  [thanks, for, lyft, credit, i, ca, n't, use, c...  \n",
       "2                            [bihday, your, majesty]  \n",
       "3  [model, i, love, u, take, with, u, all, the, t...  \n",
       "4             [factsguide, society, now, motivation]  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that no pre-processing is ideal and we have to fix some issues by ourselves\n",
    "- e.g. n't splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
      "['thanks', 'for', 'lyft', 'credit', 'i', 'ca', \"n't\", 'use', 'cause', 'they', 'do', \"n't\", 'offer', 'wheelchair', 'vans', 'in', 'pdx', 'disapointed', 'getthanked']\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[1])\n",
    "print(df.Words_normalized_no_user.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nt(words):\n",
    "    st_res = []\n",
    "    for i in range(0, len(words) - 1):\n",
    "        if words[i+1] == \"n't\" or words[i+1] == \"nt\":\n",
    "            st_res.append(words[i]+(\"n't\"))\n",
    "        else:\n",
    "            if words[i] != \"n't\" and words[i] != \"nt\":\n",
    "                st_res.append(words[i])\n",
    "    return st_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Words_normalized_no_user_fixed'] = df['Words_normalized_no_user'].apply(fix_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issue is now fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
      "['thanks', 'for', 'lyft', 'credit', 'i', 'ca', \"n't\", 'use', 'cause', 'they', 'do', \"n't\", 'offer', 'wheelchair', 'vans', 'in', 'pdx', 'disapointed', 'getthanked']\n",
      "['thanks', 'for', 'lyft', 'credit', 'i', \"can't\", 'use', 'cause', 'they', \"don't\", 'offer', 'wheelchair', 'vans', 'in', 'pdx', 'disapointed']\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[1])\n",
    "print(df.Words_normalized_no_user.iloc[1])\n",
    "print(df.Words_normalized_no_user_fixed.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_text'] = df['Words_normalized_no_user_fixed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Words</th>\n",
       "      <th>Words_normalized</th>\n",
       "      <th>Words_normalized_no_user</th>\n",
       "      <th>Words_normalized_no_user_fixed</th>\n",
       "      <th>Clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "      <td>[user, when, a, father, is, dysfunctional, and...</td>\n",
       "      <td>[when, a, father, is, dysfunctional, and, is, ...</td>\n",
       "      <td>[when, a, father, is, dysfunctional, and, is, ...</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>\n",
       "      <td>[thanks, for, lyft, credit, i, ca, n't, use, c...</td>\n",
       "      <td>[thanks, for, lyft, credit, i, can't, use, cau...</td>\n",
       "      <td>thanks for lyft credit i can't use cause they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your]</td>\n",
       "      <td>bihday your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>model i love u take with u all the time in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, now]</td>\n",
       "      <td>factsguide society now</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                               Words  \\\n",
       "0  [user, when, a, father, is, dysfunctional, and...   \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                    Words_normalized  \\\n",
       "0  [user, when, a, father, is, dysfunctional, and...   \n",
       "1  [user, user, thanks, for, lyft, credit, i, ca,...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                            Words_normalized_no_user  \\\n",
       "0  [when, a, father, is, dysfunctional, and, is, ...   \n",
       "1  [thanks, for, lyft, credit, i, ca, n't, use, c...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                      Words_normalized_no_user_fixed  \\\n",
       "0  [when, a, father, is, dysfunctional, and, is, ...   \n",
       "1  [thanks, for, lyft, credit, i, can't, use, cau...   \n",
       "2                                     [bihday, your]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4                         [factsguide, society, now]   \n",
       "\n",
       "                                          Clean_text  \n",
       "0  when a father is dysfunctional and is so selfi...  \n",
       "1  thanks for lyft credit i can't use cause they ...  \n",
       "2                                        bihday your  \n",
       "3         model i love u take with u all the time in  \n",
       "4                             factsguide society now  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the most common words in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(itertools.chain(*df.Words_normalized_no_user_fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 10136, 'to': 9798, 'a': 7680, 'i': 7165, 'you': 5679, 'and': 4868, 'in': 4615, 'for': 4462, 'of': 4133, 'is': 4048, ...})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have 34289 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34289"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The longest tweet has 42 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df.Words_normalized_no_user_fixed.apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task for the lecture\n",
    " - Try to create your own architecture using reccurent neural networks\n",
    " - Experiment a little - try different batch sizes, optimimizers, time lags as features, etc\n",
    " - Send me the Colab notebook with results and description of what you did and your final solution!\n",
    " \n",
    "# There is a competition for bonus points this week!\n",
    "- Everyone who will send me a correct solution will be included in the Accuracy toplist\n",
    "- Deadline for the competition submission is Sunday 8th November at midnigth\n",
    "- The toplist will be publicly available on Monday\n",
    "\n",
    "## The winner with the best accuracy score on test set will be awarded with 5 bonus points\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ds4_04.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
